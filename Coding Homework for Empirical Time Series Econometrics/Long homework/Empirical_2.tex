\documentclass[11pt]{article}

\oddsidemargin=17pt \evensidemargin=17pt
\headheight=9pt     \topmargin=26pt
\textheight=564pt   \textwidth=433.8pt

\usepackage{amsmath,amssymb,graphicx,color}
\usepackage{listings, hyperref}
\usepackage{graphicx}
\usepackage{subfig}

\newcommand{\excise}[1]{}
\newcommand{\comment}[1]{{$\star$\sf\textbf{#1}$\star$}}

\leftmargini=5.5ex
\leftmarginii=3.5ex

%for \marginpar to fit optimally
%hoffset=-1in
\setlength\marginparwidth{2.2in}
\setlength\marginparsep{1mm}
\newcommand\red[1]{\marginpar{\vspace{-1.4ex}\footnotesize{\color{red}#1}}}
\newcommand\score[1]{\marginpar{\vspace{-2ex}\colorbox{yellow}{#1/3}}\hspace{-1ex}}
\newcommand\total[1]{\marginpar{\colorbox{yellow}{\huge #1/42}}}
\newcommand\magenta[1]{\colorbox{magenta}{$\!$#1$\!$}}
\newcommand\yellow[1]{\colorbox{yellow}{$\!$#1$\!$}}
\newcommand\green[1]{\colorbox{green}{$\!$#1$\!$}}
\newcommand\cyan[1]{\colorbox{cyan}{$\!$#1$\!$}}
\newcommand\rmagenta[1]{\red{\magenta{\phantom{:}}\,: #1}}
\newcommand\ryellow[1]{\red{\yellow{\phantom{:}}\,: #1}}
\newcommand\rgreen[1]{\red{\green{\phantom{:}}\,: #1}}
\newcommand\rcyan[1]{\red{\cyan{\phantom{:}}\,: #1}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\mbox{}\\[-8ex]ECON612 Empirical Homework \#2, Fall 2018\\\normalsize
	Instructor: Rogier Quaedvlieg\\[-2.5ex]}
\author{Solutions by: Qingchuan Lyu\\[1ex]}
\date{Due November 20 }

\maketitle



\noindent
\textsc{Exercises}
1. (a) Let's draw graphs of series and its first difference. The plotting codes for series are the same as the code for its first difference, so I only include the codes for the graphs of first differences.
\begin{lstlisting}
>> data=readtable('Macro_Quarterly.xlsx');
>> plot(diff(data.CPI))
>> title('diff(T.CPI)')
>> plot(diff(data.GDPC1))
>> title('diff(T.GDPC1)')
>> plot(diff(data.SP500))
>> title('diff(T.SP500)')
\end{lstlisting}

Now, let's discuss which version of Dickey--Fuller test to use and provide test results and transformations. Since we could not tell the variable has zero-mean or it's just mean reverted, we have to include an intercept when we test the significance of t--statistic.

(i) GDPC1

\begin{figure}[h]%
    \centering
    \subfloat{{\includegraphics[width=6cm]{GDPC1.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=6cm]{diff(GDPC1).jpg} }}%
\end{figure}

The graph of the series has a trend. The first difference looks stable, but we are not sure. Let's test how many lags to use.
\begin{lstlisting}
>> GDPC1=data.GDPC1;
>> [h,~,~,~,reg] = adftest(GDPC1,'model','TS','lags',0:4);
>> [reg(4).tStats.t reg(4).tStats.pVal]

ans =
    2.0437    0.0421
    1.7268    0.0856
  104.8571    0.0000
    3.5235    0.0005
    3.0293    0.0027
   -0.3104    0.7565
\end{lstlisting}
From the p--value of lagged coefficients, we know that including $2$ lags is adequate. Since the coefficient of the trend is not statistically significant, we use an ARD model to conduct adf test.
\begin{lstlisting}
>> h=adftest(GDPC1, 'model', 'ARD', 'lags', 2)
h =
  logical
   0
\end{lstlisting}
From the augmented Dickey-Fuller test result, we fail to reject the null hypothesis of a unit root against the ARD alternative. Also, since we do not include a trend in our test and fail to reject the null hypothesis, we need to apply the $2$nd difference as appropriate transformation:

\begin{lstlisting}
GDPDIFF=diff(GDPC1, 2);
\end{lstlisting}

(ii) CPI

\begin{figure}[h]%
    \centering
    \subfloat{{\includegraphics[width=6cm]{CPI.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=6cm]{diff(CPI).jpg} }}%
\end{figure}

The graph of the series has a trend. Furthermore, the first difference looks very unstable. Let's test how many lags and which model to use.
\begin{lstlisting}
>> CPI=data.CPI;
>> [h,~,~,~,reg] = adftest(CPI,'model','TS','lags',0);
>> [reg(1).tStats.t reg(1).tStats.pVal]
ans =
   -0.9227    0.3571
   -3.0792    0.0023
  333.8382    0.0000
>> [h,~,~,~,reg] = adftest(CPI,'model','TS','lags',6);
>> [reg(1).tStats.t reg(1).tStats.pVal]
ans =
    1.3739    0.1708
    0.6884    0.4919
  482.5395         0
    8.5622    0.0000
   -0.7933    0.4284
    4.0400    0.0001
   -1.1329    0.2585
    1.4955    0.1362
    0.2704    0.7871
\end{lstlisting}
From the p--values, we see that the drift (constant term) is unnecessary in this model. Also, by playing with different number of lags, we see that the coefficient of the third difference lag is always near $0$, while more than three lags will produce p--values$>.05$. Therefore, $3$ lags will be sufficient for this model. Since both the constant drift and the trend coefficient are statistically insignificant when the number of lags is greater than $0$, we use an AR model to conduct adf test.
\begin{lstlisting}
>> h=adftest(GDPC1, 'model', 'AR', 'lags', 3)
h =
  logical
   0
\end{lstlisting}
From the augmented Dickey-Fuller test result, we fail to reject the null hypothesis of a unit root against the AR alternative. Also, since we do not include a trend in our test and fail to reject the null hypothesis, we need to apply the $3$rd difference as appropriate transformation:
\begin{lstlisting}
>> CPIDIFF=diff(CPI,3);
\end{lstlisting}

(iii) SP500

\begin{figure}[h]%
    \centering
    \subfloat{{\includegraphics[width=5.5cm]{SP500.jpg} }}%
    \qquad
    \subfloat{{\includegraphics[width=5.5cm]{diff(SP500).jpg} }}%
\end{figure}

The graph of the series has a trend. It looks like mean $0$ but we have to test it. 
\begin{lstlisting}
>> SP500=data.SP500;
>> [h,~,~,~,reg] = adftest(SP500,'model','TS','lags',0:2);
>> [reg(1).tStats.t reg(1).tStats.pVal]
ans =
    1.9680    0.0502
    1.8765    0.0618
   72.4454    0.0000

>> [reg(2).tStats.t reg(2).tStats.pVal]
ans =
    2.5544    0.0113
    2.4822    0.0138
   75.9281    0.0000
    5.6715    0.0000

>> [reg(3).tStats.t reg(3).tStats.pVal]
ans =
    2.3678    0.0187
    2.2950    0.0226
   75.1178    0.0000
    5.7883    0.0000
   -1.3297    0.1849
\end{lstlisting}
From the p--values of coefficients for three case of the number of lags (0 lag, 1 lag and 2 lags), we conclude that using 1 lag is adequate for this model. Furthermore, since the p--value of the constant term and trend is smaller than $0.05$ when we include one lag, I decide to use TS model to to adf test:
\begin{lstlisting}
>> h=adftest(CPI, 'model', 'TS', 'lags', 1)
h =
  logical
   0
\end{lstlisting}
From the augmented Dickey-Fuller test result, we fail to reject the null hypothesis of a unit root against the trend-stationary alternative. Also, since there is a trend and we fail to reject the unit-root null, we need to apply first difference as appropriate transformation:
\begin{lstlisting}
>> SP500DIFF=diff(SP500,1);
\end{lstlisting}

\bigskip
(b). The dimension of these differenced variables are different. We need to drop two data points from SP500DIFF and one data point from GDPDIFF randomly.
\begin{lstlisting}
>> s = RandStream('mlfg6331_64'); 
>> SP500DIFF=datasample(s,SP500DIFF,235, 'Replace',false);
>> GDPDIFF=datasample(s,GDPDIFF,235, 'Replace',false);
\end{lstlisting}
Model estimation:
\begin{lstlisting}
>> Mdl=varm(3,2);
>> EstMdl = estimate(Mdl,[GDPDIFF CPIDIFF SP500DIFF235]);
>> result=summarize(EstMdl);
>> writetable(result.Table, 'Estimation of VAR(2).xlsx','WriteRowNames',true)
\end{lstlisting}

You could read parameter estimates and t--statistics for each equation from \textit{Table 1} on the \textit{next page}. Let's compute\textit{ R--square} for each equation. I couldn't find any convenient built--in functions to compute R--square for varm. Let's infer residuals and compute R--square by hand as the following:
\begin{lstlisting}
>> E3 = infer(EstMdl,[GDPDIFF CPIDIFF SP500DIFF]);
>> y=[GDPDIFF CPIDIFF SP500DIFF];
>> Mdlfit=y(3:end,:)-E3;
>> Rsq1= 1 - sum((y(3:end,1) -Mdlfit(:,1)).^2)/sum((y(3:end,1)
- mean(y(3:end,1))).^2)
ans =
    0.0225
>> Rsq2 = 1 - sum((y(3:end,2) -Mdlfit(:,2)).^2)/sum((y(3:end,2)
- mean(y(3:end,2))).^2)
Rsq2 =
    0.4967
>> Rsq3 = 1 - sum((y(3:end,3)-Mdlfit(:,3)).^2)/sum((y(3:end,3)- mean(y(3:end,3))).^2)
Rsq3 =
    0.0176
\end{lstlisting}

Table 1 is on the \textit{next page}.

\pagebreak

    % Table generated by Excel2LaTeX 
\begin{table}[htbp]
  \centering
  \caption{Estimation Result of VAR(2)}
    \begin{tabular}{lrrrr}
    Parameters & \multicolumn{1}{l}{Value} & \multicolumn{1}{l}{StandardError} & \multicolumn{1}{l}{TStatistic} & \multicolumn{1}{l}{PValue} \\
    Constant(1) & -0.0003 & 0.0007 & -0.4192 & 0.6750 \\
    Constant(2) & 0.0001 & 0.0004 & 0.1780 & 0.8587 \\
    Constant(3) & 0.0181 & 0.0043 & 4.2462 & 0.0000 \\
    AR{1}(1,1) & -0.0093 & 0.0656 & -0.1420 & 0.8871 \\
    AR{1}(2,1) & -0.0105 & 0.0402 & -0.2613 & 0.7939 \\
    AR{1}(3,1) & 0.2824 & 0.4076 & 0.6927 & 0.4885 \\
    AR{1}(1,2) & -0.1272 & 0.0887 & -1.4337 & 0.1516 \\
    AR{1}(2,2) & -0.7938 & 0.0544 & -14.5942 & 0.0000 \\
    AR{1}(3,2) & -0.3083 & 0.5512 & -0.5593 & 0.5759 \\
    AR{1}(1,3) & 0.0148 & 0.0105 & 1.4027 & 0.1607 \\
    AR{1}(2,3) & 0.0002 & 0.0065 & 0.0333 & 0.9734 \\
    AR{1}(3,3) & -0.1016 & 0.0655 & -1.5524 & 0.1206 \\
    AR{2}(1,1) & 0.0216 & 0.0654 & 0.3307 & 0.7409 \\
    AR{2}(2,1) & 0.0090 & 0.0401 & 0.2233 & 0.8233 \\
    AR{2}(3,1) & -0.0738 & 0.4062 & -0.1816 & 0.8559 \\
    AR{2}(1,2) & -0.1430 & 0.0886 & -1.6138 & 0.1066 \\
    AR{2}(2,2) & -0.5617 & 0.0543 & -10.3389 & 0.0000 \\
    AR{2}(3,2) & 0.2406 & 0.5505 & 0.4370 & 0.6621 \\
    AR{2}(1,3) & -0.0002 & 0.0105 & -0.0188 & 0.9850 \\
    AR{2}(2,3) & -0.0048 & 0.0065 & -0.7445 & 0.4566 \\
    AR{2}(3,3) & 0.0087 & 0.0656 & 0.1321 & 0.8949 \\
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\bigskip
(c). To write this VAR in companion form, suppose $y_{1}=GDPDIFF$, $y_{2}=CPIDIFF$, and $y_{3}=SP500DIFF$. Then, we have
$$\Tilde{y_t}=\Tilde{c}+F\Tilde{y_{t-1}}+\Tilde{\epsilon_t}$$, where
\[
\Tilde{y_t}
=
\begin{bmatrix}
y_{1,t}\\
y_{2,t}\\
y_{3,t}\\
y_{1,t-1}\\
y_{2,t-1}\\
y_{3,t-1}\\
\end{bmatrix}
,\ \Tilde{c}
=
\begin{bmatrix}
-0.0003\\
0.0001\\
0.0181\\
0\\
0\\
0
\end{bmatrix}
,\ F
=
\begin{bmatrix}
-0.0093 &-0.1272 &0.0148 &0.0216 &-0.1430 &-0.0002\\
-0.0105 &-0.7938 &0.0002 &0.0090 &-0.5617 &-0.0048\\
0.2824 &-0.3083 &-0.1016 &-0.0738 &0.2406 &0.0087\\
1 &0 &0 &0 &0 &0\\
0 &1 &0 &0 &0 &0\\
0 &0 &1 &0 &0 &0
\end{bmatrix}
\]

\[
\Tilde{y}_{t-1}
=
\begin{bmatrix}
y_{1,t-1}\\
y_{2,t-1}\\
y_{3,t-1}\\
Y_{1,t-2}\\
Y_{2,t-2}\\
Y_{3,t-2}
\end{bmatrix}
,\ \Tilde{\epsilon_t}
=
\begin{bmatrix}
\epsilon_{1,t}\\
\epsilon_{2,t}\\
\epsilon_{3,t}\\
0\\
0\\
0
\end{bmatrix}.
\]
\begin{lstlisting}
>> e=eig(F) %compute eigenvalues
e =
  -0.4015 + 0.6335i
  -0.4015 - 0.6335i
   0.1120 + 0.0000i
   0.0865 + 0.0000i
  -0.1943 + 0.0000i
  -0.1060 + 0.0000i
>> abs(e) %compute the magnitude of eigenvalues
ans =
    0.7501
    0.7501
    0.1120
    0.0865
    0.1943
    0.1060
\end{lstlisting}
Since all eigenvalues of $F$ are less than $1$ in absolute value, we conclude that this process is covariance stationary.

\bigskip
(d) For individual series with p lags of all three variables, I use a loop in linear regression with three lagged variables.
\begin{lstlisting}
>> lags=1:8;
>> GDPlag=lagmatrix(y(:,1), lags);
>> CPIlag=lagmatrix(y(:,2), lags);
>> SP500lag=lagmatrix(y(:,3), lags);
AIC=zeros(8,1);
BIC=zeros(8,1);
for p=1:8
    mod=fitlm([GDPlag(:,1:p) CPIlag(:, 1:p) SP500lag(:, 1:p)],GDPDIFF);
    AIC(p,1)=mod.ModelCriterion.AIC;
    BIC(p,1)=mod.ModelCriterion.BIC;
end

AIC=zeros(8,1);
BIC=zeros(8,1);
for p=1:8
    mod=fitlm([GDPlag(:,1:p) CPIlag(:, 1:p) SP500lag(:, 1:p)], CPIDIFF);
    AIC(p,1)=mod.ModelCriterion.AIC;
    BIC(p,1)=mod.ModelCriterion.BIC;
end

AIC=zeros(8,1);
BIC=zeros(8,1);
for p=1:8
    mod=fitlm([GDPlag(:,1:p) CPIlag(:, 1:p) SP500lag(:, 1:p)],SP500DIFF);
    AIC(p,1)=mod.ModelCriterion.AIC;
    BIC(p,1)=mod.ModelCriterion.BIC;
end

\end{lstlisting}
Then, I use another loop to evaluate AIC and BIC for the whole system and extract AIC and BIC from each estimation result.
\begin{lstlisting}
p=(1:8);
estMdlResults = cell(numel(p),1); % Preallocation
for j = 1:numel(p)
    Mdl = varm(3,p(j));
    EstMdl = estimate(Mdl,y);
    estMdlResults{j} = summarize(EstMdl);
end
>> AIC=cellfun(@(x)x.AIC,estMdlResults);
>> BIC = cellfun(@(x)x.BIC,estMdlResults);
\end{lstlisting}
\begin{center}
\begin{tabular}{ |c|c|c|}
 \hline
  $p^*$ & AIC & BIC \\ 
  \hline
 GDPDIFF & 1 & 1 \\ 
 CPIDIFF & 6 & 2\\ 
 SP500DIFF235 & 1 & 1\\
  \hline
  System & 2 & 2\\
\hline
\end{tabular}
\end{center}

\bigskip
(e) Let's implement Granger causality test by hand. Note that the $5\%$ critical values for $\chi^2(2)$ is $5.99$.

\bigskip
\noindent \textit{Bivariate Granger--causality test for GDPDIFF}
\begin{lstlisting}
>>mod1=fitlm([GDPlag(:,1:2) CPIlag(:, 1:2)], GDPDIFF);
mod2=fitlm(GDPlag(:,1:2), GDPDIFF);
a1=table2array(mod1.Residuals(3:235,1));
b1=table2array(mod2.Residuals(3:235,1));
s1=233*(sum(b1.^2)-sum(a1.^2))/sum(a1.^2)
s1 =
    3.1668
p1=mod1.Coefficients.pValue(4:5) %p-value of external variables
p1 =
    0.1725
    0.1011
\end{lstlisting}
Since $3.1668<5.99$, we fail to reject the null hypothesis that CPIDIFF fails to Granger--cause GDPDIFF. Both p--values of the first lagged and the second lagged CPI variables are greater than $.05$, so the two lagged CPI variables are not statistically significant in OLS.

\bigskip
\noindent \textit{Bivariate Granger--causality test for CPIDIFF}
\begin{lstlisting}
>> mod1=fitlm([CPIlag(:,1:2) SP500lag(:, 1:2)], CPIDIFF);
mod2=fitlm(CPIlag(:,1:2), CPIDIFF);
a1=table2array(mod1.Residuals(3:235,1));
b1=table2array(mod2.Residuals(3:235,1));
s1=233*(sum(b1.^2)-sum(a1.^2))/sum(a1.^2)
p1=mod1.Coefficients.pValue(4:5) %p-value of external variables
s1 =
    0.6174
p1 =
    0.9639
    0.4443
\end{lstlisting}
Since $0.6174<5.99$, we fail to reject the null hypothesis that SP500DIFF fails to Granger--cause CPIDIFF. Both p--values of the first lagged and the second lagged SP500DIFF variables are greater than $.05$, so the two lagged SP500DIFF variables are not statistically significant in OLS.

\bigskip
\noindent \textit{Bivariate Granger--causality test for SP500DIFF}
\begin{lstlisting}
>> mod1=fitlm([SP500lag(:,1:2) GDPlag(:, 1:2)], SP500DIFF);
mod2=fitlm(SP500lag(:,1:2), SP500DIFF);
a1=table2array(mod1.Residuals(3:235,1));
b1=table2array(mod2.Residuals(3:235,1));
s1=233*(sum(b1.^2)-sum(a1.^2))/sum(a1.^2)
p1=mod1.Coefficients.pValue(4:5) %p-value of external variables
s1 =
    0.3604
p1 =
    0.5565
    0.9483
\end{lstlisting}
Since $0.3604<5.99$, we fail to reject the null hypothesis that GDPDIFF fails to Granger--cause SP500DIFF. Both p--values of the first lagged and the second lagged GDP variables are greater than $.05$, so the two lagged GDP variables are not statistically significant in OLS.

\bigskip
(f) We couldn't use the coefficients estimated in (b), since armairf function assumes constants equal to $0$. First, we need to re--estimate VAR(2) without constants.
\begin{lstlisting}
>> mdlirf=varm(3,2);
>> mdlirf.Constant=0;
>> Estmdlirf=estimate(mdlirf, y);
>> resultirf=summarize(Estmdlirf);
\end{lstlisting}

From the estimation results, we get the following coefficient matrices and innovation variance matrix:
\begin{lstlisting}
>> AR1=[-0.0085 -0.1267 0.0136; -0.0107 -0.7940 0.0005; 0.2334 -0.3367 -0.0250];
>> AR2=[0.0220 -0.1428 -0.0014; 0.0088 -0.5617 -0.0045; -0.0992 0.2283 0.0862];
>> ar0={AR1 AR2};
>> InnovCov=Estmdlirf.Covariance;
\end{lstlisting}
I use orthogonalized method to implement Cholesky decomposition and draw graphs.
\begin{lstlisting}
>> yirf = armairf(ar0,[],'Method','orthogonalized','InnovCov',...
InnovCov,'NumObs',17);
>> figure;
>> armairf(ar0,[],'Method','orthogonalized','InnovCov',InnovCov,'NumObs',17);
\end{lstlisting}

The graph is on the \textit{next page}.

\includegraphics[scale=.8]{irf.jpg}

\bigskip
(g) First, I repeat steps in (e) with a reversed order of the variables in this system.
\begin{lstlisting}
>> yrev=[SP500DIFF CPIDIFF GDPDIFF];
>> Estmdlirf2=estimate(mdlirf, yrev);
>> resultirf2=summarize(Estmdlirf2);
>> writetable(resultirf2.Table, 'Estimation without constants2.xlsx',...
'WriteRowNames',true)
%coefficients from estimation results
>> AR11=[-0.0250 -0.3367 0.2334; 0.0005 -0.7940 -0.0107; 0.0136 -0.1267 -0.0085];
>> AR22=[0.0862 0.2283 -0.0992; -0.0045 -0.5617 0.0088; -0.0014 -0.1428 0.0220];
>> ar00={AR11 AR22};
>> InnovCov2=Estmdlirf2.Covariance;
>> yirf2 = armairf(ar00,[],'Method','orthogonalized','InnovCov',...
InnovCov2,'NumObs',17);
>> figure;
>> armairf(ar00,[],'Method','orthogonalized','InnovCov',InnovCov2,...
'NumObs',17);
\end{lstlisting}
\includegraphics[scale=.8]{irf2.jpg}

If you reverse the order of variables in the second graph and compare it to the first graph, you could see that these two graphs look similar, but there are differences. For instance, when there's a shock to GDP, the impulse response of CPI is almost none when we reverse the order while it goes down a little bit in period $1$ in the original order; when there’s a shock to variable 2, the impulse response of SP500 in the second graph does not decrease as much as in the first graph during the first period; when there’s a shock to SP500, these two graphs look almost the same.

These differences are due to the method ``orthogonalized" specified in armirf function, which imposes Cholesky decomposition since innovations are contemporaneously correlated. However, Cholesky decomposition is order dependent. In the first graph, we allow GDPDIFF to contemporaneously affect CPIDIFF and SP500DIFF, and allow CPIDIFF to affect SP500DIFF, but not the reverse. In the second graph, we reverse the order, and therefore SP500DIFF could affect CPIDIFF and GDPDIFF, and CPIDIFF could affect GDPDIFF, but not the reverse. This is the reason for subtle differences in two graphs.

\bigskip
2. (a) \begin{lstlisting}
>> table2=readtable('DJIA_Returns.xlsx');
>> GE=table2.GE;
>> plot(GE)
>> title('GE')
\end{lstlisting}
\includegraphics[scale=.6]{GE.jpg}

\bigskip
(b). I first use arima and generate residuals to test whether they are serially autocorrelated.
\begin{lstlisting}
>> model = arima(2,0,0);
>> EstMdl2=estimate(model, GE);
\end{lstlisting}
I use Ljung-box test to figure out whether there's serial autocorrelation among residuals. lbqtest is appropriate for a series with a constant mean. Since the series appears to fluctuate around a constant mean in the plot in (a), I do not need to transform the data.
\begin{lstlisting}
>> [E,~]=infer(EstMdl2, GE);
>> [h,pValue] = lbqtest(E) 
h =
  logical
   1
pValue =
   4.1078e-10
\end{lstlisting}
SInce $h=1$ with p--value almost $0$, we reject the null hypothesis that residuals are not serially autocorrelated. Let's apply Newey--West test to compute appropriate standard errors:
\begin{lstlisting}
>> T = hours(1:4528);
>> TGE=timetable(T', GE);
>> GE1=lag(TGE,1);
>> GE2=lag(TGE,2);
>> ARGE1=GE1{:,1};
>> ARGE2=GE2{:,1};
>> X=[ARGE1, ARGE2];
>> hac(X,GE,'intercept',true,'display','full');
>> % compute t-statistic=coeff/SE
>> tstat=[0.0001/0.0003 -0.0088/0.0273 -0.0127/0.0306]
>> % compute R-square
>> GEfit=GE-E;
>> Rsq = 1 - sum((GE - GEfit).^2)/sum((GE... 
- mean(GE)).^2)
\end{lstlisting}
The result of my estimation with Newey-West standard errors is
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 & Estimate & Std error & t-statistic \\ 
 \hline
 $\phi_0$ & 0.0001 & 0.0003 & 0.3333  \\ 
 $\phi_1$ & -0.0088 & 0.0273 & -0.3223\\ 
 $\phi_2$ & -0.0127 & 0.0306 & -0.4150\\
 \hline
 $R^2$ &\multicolumn{3}{c|}{ 0.00023 }\\
 \hline
\end{tabular}
\end{center}

\bigskip
(c)
\begin{lstlisting}
>> MdgGARCH=garch(1,1);
>> EstMdlGARCH=estimate(MdgGARCH, E);
\end{lstlisting}

\bigskip
(d)
\begin{lstlisting}
>> Mdgjr=gjr(1,1);
>> EstMdlgjr=estimate(Mdgjr, E);
>> [EstMdlGARCH,EstParamCov,rlogL,info]=estimate(MdgGARCH, E);
>> [EstMdlgjr,EstParamCov,ulogL,info]=estimate(Mdgjr, E);
\end{lstlisting}
\begin{tabular}{ |c|c|c|c|c|c|c|  }
 \hline
  &\multicolumn{3}{c|}{GARCH} &\multicolumn{3}{c|}{GJR-GARCH} \\
  \hline
  & Estimate & Std error & t-statistic & Estimate & Std error & t-statistic \\ 
  \hline
 $\omega$ & 2.66862e-06 &  4.20872e-07 & 6.34068 &2.25773e-06   & 3.90658e-07 & 5.77931 \\ 
 $\alpha$ & 0.908987 & 0.00418633 & 217.132 & 0.928717 & 0.00437941 & 212.064\\ 
 $\beta$ & 0.0852821 & 0.00436564 & 19.5349 & 0.0300692 & 0.00333381 & 9.01947\\
 $\gamma$ &  & &  & 0.0719852 & 0.00655062 & 10.9891\\
  \hline
  LogLik &\multicolumn{3}{c|}{1.2618e+04 } & \multicolumn{3}{c|}{1.2644e+04 }\\
\hline
\end{tabular}

\bigskip
(e) My unrestricted model is GJR--GARCH and my restricted model is GARCH. Since only $\gamma$ is restricted, the degree of freedom in my likelihood ratio test is $1$. The codes of ulogL and rlogL are in my answer to (d).
\begin{lstlisting}
>> [h,pValue] = lratiotest(ulogL,rlogL,1)
h =
  logical
   1
pValue =
   3.9824e-13
\end{lstlisting}
Since $h=1$ and p--value is almost $0$, I reject the null, restricted model in favor of the alternative, unrestricted model. Therefore, GJR--GARCH model is preferred.

\bigskip
(f).\begin{lstlisting}
>> V1=infer(EstMdlGARCH, E);
>> SE1=sqrt(V1)*sqrt(252);
>> V2=infer(EstMdlgjr, E);
>> SE2=sqrt(V2)*sqrt(252);
>> plot(SE1,'r')
>> hold on
>> plot(SE2,'k:','LineWidth',1.5)
>> legend('GARCH','GJR-GARCH')
>> title('GE')
\end{lstlisting}

The graph is on the \textit{next page}.

\includegraphics[scale=.8]{CV_resid.jpg}

\end{document}

