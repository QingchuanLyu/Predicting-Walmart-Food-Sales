{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgment\n",
    "-[m5 baseline](https://www.kaggle.com/harupy/m5-baseline) by harupy\n",
    "\n",
    "-[Back to (predict) the future - Interactive M5 EDA](https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda) by headsortails\n",
    "\n",
    "-[123rd place solution](https://www.kaggle.com/xxbxyae/123rd-place-solution) by Kien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "register_matplotlib_converters()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    INPUT_DIR = f\"/kaggle/input/m5-forecasting-accuracy\"\n",
    "\n",
    "    calendar = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\").pipe(reduce_mem_usage)\n",
    "    prices = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\").pipe(reduce_mem_usage)\n",
    "\n",
    "\n",
    "    sales = pd.read_csv(f\"{INPUT_DIR}/sales_train_evaluation.csv\",).pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "    submission = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\").pipe(\n",
    "        reduce_mem_usage\n",
    "    )\n",
    "\n",
    "    print(\"sales shape:\", sales.shape)\n",
    "    print(\"prices shape:\", prices.shape)\n",
    "    print(\"calendar shape:\", calendar.shape)\n",
    "    print(\"submission shape:\", submission.shape)\n",
    "\n",
    "\n",
    "    return sales, prices, calendar, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "sales, prices, calendar, submission = read_data()\n",
    "\n",
    "NUM_ITEMS = sales.shape[0]  # 30490\n",
    "DAYS_PRED = submission.shape[1] - 1  # 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar = encode_categorical(\n",
    "    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "sales = encode_categorical(\n",
    "    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "prices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def extract_num(ser):\n",
    "    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n",
    "\n",
    "\n",
    "def reshape_sales(sales, submission, d_thresh=0, verbose=True):\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "\n",
    "    evals_columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "    # separate test dataframes.\n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    # get product table.\n",
    "    \n",
    "    \n",
    "    product = sales[id_columns]\n",
    "    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n",
    "    sales = reduce_mem_usage(sales)\n",
    "\n",
    "\n",
    "    evals.columns = evals_columns#[\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "\n",
    "    # merge with product table\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"evaluation\")\n",
    "        display(evals)\n",
    "\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    sales[\"part\"] = \"train\"\n",
    "    #sales[\"part\"][~(sales[\"d\"]<1914)] = \"validation\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "\n",
    "    data = pd.concat([sales, evals], axis=0)\n",
    "\n",
    "    del sales, evals\n",
    "\n",
    "    data[\"d\"] = extract_num(data[\"d\"])\n",
    "    data = data[data[\"d\"] >= d_thresh]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data\")\n",
    "        display(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_calendar(data, calendar):\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "\n",
    "def merge_prices(data, prices):\n",
    "    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "data = reshape_sales(sales, submission, d_thresh=1941 - int(365 * 2  + 7))\n",
    "del sales\n",
    "gc.collect()\n",
    "\n",
    "calendar[\"d\"] = extract_num(calendar[\"d\"])\n",
    "data = merge_calendar(data, calendar)\n",
    "del calendar\n",
    "gc.collect()\n",
    "\n",
    "data = merge_prices(data, prices)\n",
    "del prices\n",
    "gc.collect()\n",
    "\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def add_demand_features(df):\n",
    "    for diff in [0, 56]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "    print(\"shift done\")\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    diff = 56\n",
    "    for window in [56, 112, 168]:\n",
    "        df[f\"shift_t{diff}_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).std()\n",
    "        )\n",
    "        df[f\"shift_t{diff}_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).mean()\n",
    "        )\n",
    "        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).min()\n",
    "        )\n",
    "        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).max()\n",
    "        )\n",
    "        df[f\"rolling_sum_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).sum()\n",
    "        )\n",
    "    print(\"window done\")\n",
    "    df[\"rolling_skew_t56\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(28).skew()\n",
    "    )\n",
    "    df[\"rolling_kurt_t56\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(28).kurt()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_features(df):\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n",
    "\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    attrs = [\n",
    "        \"year\",\n",
    "        \"quarter\",\n",
    "        \"month\",\n",
    "        \"week\",\n",
    "        \"day\",\n",
    "        \"dayofweek\",\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    df['month_day']  = df['month'] * 100 + df['day']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "print('add_demand_features done')\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "print('add_price_features done')\n",
    "data = add_original_features(data).pipe(reduce_mem_usage)\n",
    "print('add_original_features done')\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")\n",
    "\n",
    "print(\"start date:\", data[dt_col].min())\n",
    "print(\"end date:\", data[dt_col].max())\n",
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "class CustomTimeSeriesSplitter:\n",
    "    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n",
    "        self.n_splits = n_splits\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.day_col = day_col\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        SEC_IN_DAY = 3600 * 24\n",
    "        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n",
    "        duration = sec.max()\n",
    "\n",
    "        train_sec = self.train_days * SEC_IN_DAY\n",
    "        test_sec = self.test_days * SEC_IN_DAY\n",
    "        total_sec = test_sec + train_sec\n",
    "\n",
    "        if self.n_splits == 1:\n",
    "            train_start = duration - total_sec\n",
    "            train_end = train_start + train_sec\n",
    "\n",
    "            train_mask = (sec >= train_start) & (sec < train_end)\n",
    "            test_mask = sec >= train_end\n",
    "\n",
    "            yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "        else:\n",
    "            # step = (duration - total_sec) / (self.n_splits - 1)\n",
    "            step = DAYS_PRED * SEC_IN_DAY\n",
    "\n",
    "            for idx in range(self.n_splits):\n",
    "                # train_start = idx * step\n",
    "                shift = (self.n_splits - (idx + 1)) * step\n",
    "                train_start = duration - total_sec - shift\n",
    "                train_end = train_start + train_sec\n",
    "                test_end = train_end + test_sec\n",
    "\n",
    "                train_mask = (sec > train_start) & (sec <= train_end)\n",
    "\n",
    "                if idx == self.n_splits - 1:\n",
    "                    test_mask = sec > train_end\n",
    "                else:\n",
    "                    test_mask = (sec > train_end) & (sec <= test_end)\n",
    "\n",
    "                yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "day_col = \"d\"\n",
    "cv_params = {\n",
    "    \"n_splits\": 5,\n",
    "    \"train_days\": 365,\n",
    "    \"test_days\": 28,#DAYS_PRED,\n",
    "    \"day_col\": day_col,\n",
    "}\n",
    "cv = CustomTimeSeriesSplitter(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "    \"sell_price\",\n",
    "    # demand features\n",
    "    \"shift_t28\",\n",
    "    \"shift_t56\",\n",
    "    # std\n",
    "    \"shift_t28_rolling_std_t28\",\n",
    "    \"shift_t28_rolling_std_t56\",\n",
    "    \"shift_t28_rolling_std_t84\",\n",
    "    # mean\n",
    "    \"shift_t28_rolling_mean_t28\",\n",
    "    \"shift_t28_rolling_mean_t56\",\n",
    "    \"shift_t28_rolling_mean_t84\",\n",
    "    # min,\n",
    "    \"rolling_min_t28\",\n",
    "    # max\n",
    "    \"rolling_max_t28\",\n",
    "    \"rolling_max_t56\",\n",
    "    # sum\n",
    "    \"rolling_sum_t28\",    \n",
    "    \"rolling_sum_t56\",        \n",
    "    \"rolling_kurt_t28\",    \n",
    "    \"price_change_t365\",\n",
    "    \"rolling_price_std_t30\",\n",
    "    # time features\n",
    "    \"year\",\n",
    "    \"quarter\",\n",
    "    \"month\",\n",
    "    \"week\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"is_weekend\",\n",
    "    \"month_day\",\n",
    "    # original features\n",
    "    \"shift_t28_log\",\n",
    "    \"shift_t28_sqrt\",\n",
    "    \"shift_t56_log\",\n",
    "    \"shift_t56_sqrt\",\n",
    "    \"shift_t28_diff_t7\",\n",
    "    'shift_t56_diff_t7'\n",
    "    #\"self_diff_t7\",\n",
    "    #\"self_diff_t28\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "is_train = (data[\"d\"] < 1942)# & (data[\"d\"]>=1544)\n",
    "\n",
    "is_private = (data[\"d\"] >= 1942)\n",
    "is_public = ~(data[\"d\"] < 1914) & ~(is_private)\n",
    "\n",
    "\n",
    "# Attach \"d\" to X_train for cross validation.\n",
    "X_train = data[is_train][[day_col] + features].reset_index(drop=True)\n",
    "y_train = data[is_train][\"demand\"].reset_index(drop=True)\n",
    "X_test_pub = data[is_public][features].reset_index(drop=True)\n",
    "X_test_pri = data[is_private][features].reset_index(drop=True)\n",
    "\n",
    "# keep these two columns to use later.\n",
    "id_date_pub = data[is_public][[\"id\", \"date\"]].reset_index(drop=True)\n",
    "id_date_pri = data[is_private][[\"id\", \"date\"]].reset_index(drop=True)\n",
    "\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "print(X_train['d'][0])\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test_pub shape:\", X_test_pub.shape)\n",
    "print(\"X_test_pri shape:\", X_test_pri.shape)\n",
    "print(\"id_date_pub shape:\", id_date_pub.shape)\n",
    "print(\"id_date_pri shape:\", id_date_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n",
    "    models = []\n",
    "\n",
    "    if drop_when_train is None:\n",
    "        drop_when_train = []\n",
    "\n",
    "    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n",
    "        print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n",
    "\n",
    "        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n",
    "        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n",
    "        \n",
    "        print(f'\\n train d min: {X_trn[\"d\"].min()} \\n valid d min: {X_val[\"d\"].min()} \\n')\n",
    "        \n",
    "        train_set = lgb.Dataset(\n",
    "            X_trn.drop(drop_when_train, axis=1),\n",
    "            label=y_trn,\n",
    "            categorical_feature=[\"item_id\"],\n",
    "        )\n",
    "        val_set = lgb.Dataset(\n",
    "            X_val.drop(drop_when_train, axis=1),\n",
    "            label=y_val,\n",
    "            categorical_feature=[\"item_id\"],\n",
    "        )\n",
    "        eval_result = {}\n",
    "        model = lgb.train(\n",
    "            bst_params,\n",
    "            train_set,\n",
    "            valid_sets=[train_set, val_set],\n",
    "            valid_names=[\"train\", \"valid\"],\n",
    "            evals_result=eval_result,\n",
    "            **fit_params,\n",
    "        )\n",
    "        models.append(model)\n",
    "\n",
    "        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    return models, eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "bst_params =  {'lambda_l1': 0.00021301070633699974,\n",
    "               'lambda_l2': 9.242591106708853e-07,\n",
    "               'num_leaves': 31, \n",
    "               'feature_fraction': 0.584,\n",
    "               'bagging_fraction': 1.0, \n",
    "               'bagging_freq': 0, \n",
    "               'min_child_samples': 20, \n",
    "               'boosting_type': 'gbdt',\n",
    "               'metric': 'rmse',\n",
    "               'objective': 'poisson',\n",
    "               'n_jobs': -1,\n",
    "               'seed': 42,\n",
    "               'learning_rate': 0.03,\n",
    "               'min_data_in_leaf': 20}\n",
    "\n",
    "fit_params = {\n",
    "    \"num_boost_round\": 100_000,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"verbose_eval\": 100,\n",
    "}\n",
    "\n",
    "models, evals = train_lgb(\n",
    "    bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n",
    ")\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "Put the highest weight on the most recent timeseries model(fold 5) and put smaller weights on other folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "imp_type = \"gain\"\n",
    "importances = np.zeros(X_test_pub.shape[1])\n",
    "preds_pub = np.zeros(X_test_pub.shape[0])\n",
    "preds_pri = np.zeros(X_test_pri.shape[0])\n",
    "\n",
    "for model in models:\n",
    "    importances += model.feature_importance(imp_type)\n",
    "\n",
    "preds_pub = 0.6*models[5].predict(X_test_pub)+0.3*models[1].predict(X_test_pub)+0.1*models[4].predict(X_test_pub)\n",
    "preds_pri = 0.6*models[5].predict(X_test_pri)+0.3*models[1].predict(X_test_pri)+0.1*models[4].predict(X_test_pri)\n",
    "importances = importances / cv.get_n_splits()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
