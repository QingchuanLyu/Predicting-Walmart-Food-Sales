{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis with LightGBM\n",
    "\n",
    "Question: how could you estimate the point forecasts of the unit sales of products at Walmart in the U.S.?\n",
    "\n",
    "Input datasets are from Kaggle. I estimated item sales at stores in various locations for two 28-day time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "register_matplotlib_converters()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions and Input Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#define a helper function to reduce memory sizes as the original datasets are large\n",
    "def reduce_mem_usage(df, verbose=False):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#define a function to read in data and reduce memory size\n",
    "def read_data():\n",
    "    calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\").pipe(reduce_mem_usage)\n",
    "    prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\").pipe(reduce_mem_usage)\n",
    "    sales = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\",).pipe(reduce_mem_usage)\n",
    "    sample_eval = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\").pipe(reduce_mem_usage)\n",
    "\n",
    "    return sales, prices, calendar, sample_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#read in three original datasets and reduce sizes with pipe\n",
    "sales, prices, calendar, sample_eval= read_data()\n",
    "#check how many items to predict\n",
    "NUM_ITEMS = sales.shape[0]  # 30490\n",
    "#predicting 28 days of sales\n",
    "DAYS_PRED = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#convert categorical vars to numerical vars\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "    return df\n",
    "\n",
    "#apply label encoding to event names/types in calendar\n",
    "calendar = encode_categorical(\n",
    "                            calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    "                             ).pipe(reduce_mem_usage)\n",
    "\n",
    "#apply label encoding to ids in sales and prices\n",
    "sales = encode_categorical(\n",
    "                        sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                         ).pipe(reduce_mem_usage)\n",
    "\n",
    "prices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#define a helper function to extract groups of digits\n",
    "def extract_num(ser):\n",
    "    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n",
    "\n",
    "#concatenate train and evaluation data\n",
    "#extract ids and generate predicting ids\n",
    "def reshape_sales(sales, submission, d_thresh=0, verbose=True):\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    #concatenate id columns with day numbers\n",
    "    evals_columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "    #separate test data: return if id variable ends with evaluation\n",
    "    evals = sample_eval[sample_eval[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    #generate a table with ids. a pivot table of demands, and a col of eval ids\n",
    "    product = sales[id_columns]\n",
    "    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n",
    "    sales = reduce_mem_usage(sales)\n",
    "    evals.columns = evals_columns\n",
    "\n",
    "    #merge evals with product table--a table of ids\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "    #generate a pivot tbale simialr to sales\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "    #claim train and evaluation data then stack together\n",
    "    sales[\"part\"] = \"train\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "    data = pd.concat([sales, evals], axis=0)\n",
    "\n",
    "    #extract id numbers and drop data out of range\n",
    "    data[\"d\"] = extract_num(data[\"d\"])\n",
    "    data = data[data[\"d\"] >= d_thresh]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop redundant time variables and add the rest to the main dataframe\n",
    "def merge_calendar(data, calendar):\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "#add price variables to the main dataframe\n",
    "def merge_prices(data, prices):\n",
    "    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#create a concatenated dataframe inside the study window\n",
    "data = reshape_sales(sales, sample_eval, d_thresh=1941 - int(365 * 2  + 7))\n",
    "#only keep numbers of time variables\n",
    "calendar[\"d\"] = extract_num(calendar[\"d\"])\n",
    "#add calendar variables to the main data frame\n",
    "data = merge_calendar(data, calendar)\n",
    "#add prices to the main dataframe\n",
    "data = merge_prices(data, prices)\n",
    "#reduce memory usage\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#create demand features: rolling window summary statistics\n",
    "def add_demand_features(df):\n",
    "    #28 days to predict\n",
    "    for diff in [0, 28]:\n",
    "        shift = DAYS_PRED + diff\n",
    "        #use shift func to setup a rolling window\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "    \n",
    "    diff = 28\n",
    "    #roll features: summary statistics\n",
    "    for window in [28, 56, 84]:\n",
    "        df[f\"shift_t{diff}_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).std()\n",
    "        )\n",
    "        df[f\"shift_t{diff}_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).mean()\n",
    "        )\n",
    "        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).min()\n",
    "        )\n",
    "        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).max()\n",
    "        )\n",
    "        df[f\"rolling_sum_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).sum()\n",
    "        )\n",
    "    #measure asymmetry\n",
    "    df[\"rolling_skew_t56\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(56).skew()\n",
    "    )\n",
    "    #measure tailedness\n",
    "    df[\"rolling_kurt_t56\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "        lambda x: x.shift(DAYS_PRED).rolling(56).kurt()\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create price features by setting up rolling window summary statistics\n",
    "def add_price_features(df):\n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract time features\n",
    "def add_time_features(df, dt_col):\n",
    "    #convert to datetime varible on the purpose of attribute extraction\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    attrs = [\n",
    "            \"year\",\n",
    "            \"quarter\",\n",
    "            \"month\",\n",
    "            \"week\",\n",
    "            \"day\",\n",
    "            \"dayofweek\"\n",
    "            ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        #save space\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        #get the value of attribute of time varible\n",
    "        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n",
    "    #additional time variables that require manipulation\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "    df['month_day']  = df['month'] * 30 + df['day']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#apply helper func to create features\n",
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "data = add_original_features(data).pipe(reduce_mem_usage)\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#sklearn.model_selection.TimeSeriesSplit doesn't work well; create a customized time series splitter\n",
    "class CustomTimeSeriesSplitter:\n",
    "    #initialize macro variables; col name of date is \"d\"\n",
    "    def __init__(self, n_splits=5, train_days=365, test_days=28, day_col=day_col):\n",
    "        self.n_splits = n_splits\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.day_col = day_col\n",
    "    #specify the beginning and end index values of each fold\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        #seconds in a day\n",
    "        SEC_IN_DAY = 3600 * 24\n",
    "        #calculate the time range wrt the first row (initial time point)\n",
    "        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n",
    "        duration = sec.max()\n",
    "        \n",
    "        #total secs of train and test data\n",
    "        train_sec = self.train_days * SEC_IN_DAY\n",
    "        test_sec = self.test_days * SEC_IN_DAY\n",
    "        total_sec = test_sec + train_sec\n",
    "        #if don't use cross-validation\n",
    "        if self.n_splits == 1:\n",
    "            train_start = duration - total_sec\n",
    "            train_end = train_start + train_sec\n",
    "            #check inside data range\n",
    "            train_mask = (sec >= train_start) & (sec < train_end)\n",
    "            test_mask = sec >= train_end\n",
    "            #use yield to iterate over a series; return index values\n",
    "            yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "        #if use cross-validation\n",
    "        else:\n",
    "            #window size = prediction size\n",
    "            step = DAYS_PRED * SEC_IN_DAY\n",
    "            #for each split\n",
    "            for idx in range(self.n_splits):\n",
    "                #compute rolling train data for each split; test data follows 28 days later\n",
    "                shift = (self.n_splits - (idx + 1)) * step\n",
    "                train_start = duration - total_sec - shift\n",
    "                train_end = train_start + train_sec\n",
    "                test_end = train_end + test_sec\n",
    "                #check inside data range\n",
    "                train_mask = (sec > train_start) & (sec <= train_end)\n",
    "                #if not the last split, check data range\n",
    "                if idx ^= self.n_splits - 1:\n",
    "                    test_mask = (sec > train_end) & (sec <= test_end)\n",
    "                #if the last split, don't need to check the end of test mask\n",
    "                else:\n",
    "                    test_mask = sec > train_end\n",
    "                #use yield to iterate over a series; return index values\n",
    "                yield sec[train_mask].index.values, sec[test_mask].index.values\n",
    "\n",
    "    #a simple helper func to tell how many folds\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "day_col = \"d\"\n",
    "#build a dictionary of parameters\n",
    "cv_params = {\n",
    "    \"n_splits\": 5,\n",
    "    \"train_days\": 365,\n",
    "    \"test_days\": 28,#DAYS_PRED,\n",
    "    \"day_col\": day_col,\n",
    "}\n",
    "#use ** to call dictionary values\n",
    "cv = CustomTimeSeriesSplitter(**cv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#selected part of features by experimentation\n",
    "features = [\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"event_name_1\",\n",
    "    \"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    \"event_type_2\",\n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "    \"sell_price\",\n",
    "    # demand features\n",
    "    \"shift_t28\",\n",
    "    \"shift_t56\",\n",
    "    # std\n",
    "    \"shift_t28_rolling_std_t28\",\n",
    "    \"shift_t28_rolling_std_t56\",\n",
    "    \"shift_t28_rolling_std_t84\",\n",
    "    # mean\n",
    "    \"shift_t28_rolling_mean_t28\",\n",
    "    \"shift_t28_rolling_mean_t56\",\n",
    "    \"shift_t28_rolling_mean_t84\",\n",
    "    # min,\n",
    "    \"rolling_min_t28\",\n",
    "    # max\n",
    "    \"rolling_max_t28\",\n",
    "    \"rolling_max_t56\",\n",
    "    # sum\n",
    "    \"rolling_sum_t28\",    \n",
    "    \"rolling_sum_t56\",        \n",
    "    \"rolling_kurt_t28\",    \n",
    "    \"price_change_t365\",\n",
    "    \"rolling_price_std_t30\",\n",
    "    # time features\n",
    "    \"year\",\n",
    "    \"quarter\",\n",
    "    \"month\",\n",
    "    \"week\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    \"is_weekend\",\n",
    "    \"month_day\",\n",
    "    # original features\n",
    "    \"shift_t28_log\",\n",
    "    \"shift_t28_sqrt\",\n",
    "    \"shift_t56_log\",\n",
    "    \"shift_t56_sqrt\",\n",
    "    \"shift_t28_diff_t7\",\n",
    "    'shift_t56_diff_t7'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate training data as that before d1942\n",
    "is_train = (data[\"d\"] < 1942)# & (data[\"d\"]>=1544)\n",
    "#separate test data as that after d1942\n",
    "is_test = (data[\"d\"] >= 1942)\n",
    "\n",
    "# Attach \"d\" (id variable from the original dataset) to X_train for cross validation\n",
    "X_train = data[is_train][[day_col] + features].reset_index(drop=True)\n",
    "#apply feature seelction and reset index\n",
    "y_train = data[is_train][\"demand\"].reset_index(drop=True)\n",
    "X_test = data[is_test][features].reset_index(drop=True)\n",
    "\n",
    "# keep id and date variables for future use\n",
    "id_date_pri = data[is_test][[\"id\", \"date\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#Define a function of training lightGBM\n",
    "#use poisson objective function as the goal is to predict sales amount\n",
    "\n",
    "#parameters used in lgb training\n",
    "bst_params =  {'lambda_l1': 0.00021301070633699974,\n",
    "               'lambda_l2': 9.242591106708853e-07,\n",
    "               'num_leaves': 31, \n",
    "               'feature_fraction': 0.584,\n",
    "               'bagging_fraction': 1.0, \n",
    "               'bagging_freq': 0, \n",
    "               'min_child_samples': 20, \n",
    "               'boosting_type': 'gbdt',\n",
    "               'metric': 'rmse',\n",
    "               'objective': 'poisson',\n",
    "               'n_jobs': -1,\n",
    "               'seed': 42,\n",
    "               'learning_rate': 0.03,\n",
    "               'min_data_in_leaf': 20}\n",
    "\n",
    "#define models\n",
    "def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n",
    "    models = []\n",
    "    #in case of not dropping anything\n",
    "    if drop_when_train is None:\n",
    "        drop_when_train = []\n",
    "\n",
    "    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n",
    "        #print fold index / total number of folds\n",
    "        print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n",
    "        \n",
    "        #print the min d var of train and test (begining and end of training data)\n",
    "        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n",
    "        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]        \n",
    "        print(f'\\n train d min: {X_trn[\"d\"].min()} \\n valid d min: {X_val[\"d\"].min()} \\n')\n",
    "        \n",
    "        #initialize train dataset\n",
    "        train_set = lgb.Dataset(\n",
    "                                X_trn.drop(drop_when_train, axis=1), #main dataset to train on\n",
    "                                label=y_trn, #label of dataset\n",
    "                                categorical_feature=[\"item_id\"], #list of int: indices\n",
    "                                )\n",
    "        #initialize validation dataset\n",
    "        val_set = lgb.Dataset(\n",
    "                                X_val.drop(drop_when_train, axis=1),\n",
    "                                label=y_val,\n",
    "                                categorical_feature=[\"item_id\"],\n",
    "                             )\n",
    "              \n",
    "        #perform training with parameters\n",
    "        model = lgb.train(\n",
    "                            bst_params, #given parameters\n",
    "                            train_set, #data to be trained on\n",
    "                            valid_sets=[train_set, val_set], #data to be evaluated during training\n",
    "                            valid_names=[\"train\", \"valid\"], #names of valid sets\n",
    "                            evals_result=eval_result, #a dictionary to store all the evaluated results\n",
    "                            num_boost_round= 100_000, #number of boosting iterations: 100000\n",
    "                            early_stopping_rounds= 100, #validation score needs to improve at least every 100 round(s) to continue training\n",
    "                            verbose_eval= 100, #int: the evaluation metric is printed at each 100 round\n",
    "                            )\n",
    "        #stack validated models\n",
    "        models.append(model)\n",
    "        #clean subset of data for the next fold\n",
    "        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n",
    "        gc.collect()\n",
    "\n",
    "    return models, eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#output models and evaluation\n",
    "models, evals = train_lgb(\n",
    "                        bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "I'm using a weighted average meta model: put the highest weight on the most recent timeseries model(fold 5) and put smaller weights on other folds according to evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "preds = np.zeros(X_test.shape[0])\n",
    "preds = 0.6*models[4].predict(X_test)+0.3*models[1].predict(X_test)+0.1*models[3].predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
